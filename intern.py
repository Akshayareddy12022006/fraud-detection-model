# -*- coding: utf-8 -*-
"""intern.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18E0UolvxVjZO8UYLDCx2dHafFi2bpKfk
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt

train_df = pd.read_csv('fraudTrain.csv', nrows=100000)
test_df = pd.read_csv('fraudTest.csv', nrows=50000)

print("Train data shape:", train_df.shape)
print("Test data shape:", test_df.shape)

def preprocess(df):
    # Convert 'trans_date_trans_time' to datetime
    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])
    df['hour'] = df['trans_date_trans_time'].dt.hour
    df['day'] = df['trans_date_trans_time'].dt.dayofweek

    # Drop unnecessary columns
    drop_cols = ['trans_date_trans_time', 'cc_num', 'first', 'last', 'street', 'city', 'state', 'zip', 'unix_time', 'dob', 'merchant', 'job']
    df.drop(columns=drop_cols, inplace=True)

    # Encode categorical features
    cat_cols = ['category', 'gender']
    for col in cat_cols:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])

    # Ensure all columns are numeric
    df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

    return df

train_df = preprocess(train_df)
test_df = preprocess(test_df)

X_train = train_df.drop(columns=['is_fraud'])
y_train = train_df['is_fraud']

X_test = test_df.drop(columns=['is_fraud'])
y_test = test_df['is_fraud']

# 5. CHECK FOR MISSING AND INFINITE VALUES BEFORE SCALING
# Check for missing values in X_train and X_test
print("Missing values in X_train:")
print(X_train.isna().sum())
print("Missing values in X_test:")
print(X_test.isna().sum())

# Handle missing values (fill with mean for numerical columns)
X_train = X_train.fillna(X_train.mean())
X_test = X_test.fillna(X_test.mean())

# Check for infinite values
print("Infinite values in X_train:", np.isinf(X_train).sum())
print("Infinite values in X_test:", np.isinf(X_test).sum())

# Replace infinite values with NaN and then fill them
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

# Handle NaN values after replacing infinite values
X_train = X_train.fillna(X_train.mean())
X_test = X_test.fillna(X_test.mean())

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit on X_train, transform X_train
X_test_scaled = scaler.transform(X_test)  # Only transform X_test using the already fitted scaler

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)
rf_preds = rf_model.predict(X_test_scaled)

print("Random Forest Performance:\n")
print(classification_report(y_test, rf_preds))
print("ROC-AUC Score:", roc_auc_score(y_test, rf_model.predict_proba(X_test_scaled)[:, 1]))

xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', max_depth=4)
xgb_model.fit(X_train_scaled, y_train)
xgb_preds = xgb_model.predict(X_test_scaled)

print("\nXGBoost Performance:\n")
print(classification_report(y_test, xgb_preds))
print("ROC-AUC Score:", roc_auc_score(y_test, xgb_model.predict_proba(X_test_scaled)[:, 1]))

def plot_conf_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(title)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

plot_conf_matrix(y_test, rf_preds, "Random Forest Confusion Matrix")
plot_conf_matrix(y_test, xgb_preds, "XGBoost Confusion Matrix")

 